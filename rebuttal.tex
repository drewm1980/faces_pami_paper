\documentclass[11pt]{article}

\usepackage{epsfig}
\usepackage{latexsym, amsmath, amsfonts}
%\usepackage{pdfsync}

\renewcommand{\baselinestretch}{1.0}
\parskip 2.2mm
\parindent 0mm
\topmargin -0.60in \oddsidemargin 0.0625in \textheight 9.00in
\textwidth 6.50in

\renewcommand{\Re}{{\mathbb R}}
\newcommand{\Ze}{{\mathbb Z}}
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}
\newenvironment{proof}{{\bf Proof: \ }}{ \hfill \QED}

\newcommand{\ie}{{\it i.e., }}
\newcommand{\D}{\displaystyle}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{fact}{Fact}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}



\begin{document}

   \bibliographystyle{plain}

   \title{\Large {\bf Manuscript  Revision Statement}}

   \author{{Andrew Wagner, John Wright, Arvind Ganesh, Zihan Zhou, Hossein Mobahi, and Yi Ma}}

   \date{}
   \maketitle

We again thank the reviewers and the area chair for their comments.
%I am mostly satisfied with the responses to the earlier review. The paper is
%in much better form now and more carefully written.
Changes have been made to the paper to address all of the changes requested by Reviewer 1,
including a more complete set of results in Table 2, Table 3, Figure 11, and associated
discussions.  We give a quick overview of the changes.

{\bf
	Page 1 line 16; "there are many face recognition that fall roughly.."
	missing the word 'applications'
}

The typo on Page 1 is now amended.

{\bf
	In Table 1, the baseline results are shown with subscripts 'd' and 'm', i.e.
	using results of a detector, and manual registration. It seems like an obvious
	omission not to include results of recognition where input comes from the
	proposed registration algorithm itself. While the paper states that even
	manual registration works worse, it is not particularly convincing that manual
	registration is to be considered better than automatic registration - the
	human visual sensitivity is pretty bad for a problem like registration.
}

We ran the requested experiments.  We agree with the reviewer that automatic registration
can often out-perform manual registration.  The results back this up, and the text
is amended to be more clear on this point.  The new more complete benchmark is shown in
Table 2, and Figure 11.  All of the classical methods benefit from using our iterative
alignment procedure.  This is even true for LBP, which is known to have some robustness
to moderate misalignment.  The discussions have been updated to cover the new results.

{\bf
	In any case, the extent of manual input is unclear. It is said 'the input face
	is aligned to the training with manually selected outer eye corners'. If only
	2 points are manually selected, then what kind of registration can be done
	with just 2 corresponding points ? Surely not affine alignment. Many more are
	needed if it is to be done robustly. So it is not surprising that manual
	alignment is giving worse results than automatic. If my interpretation is
	wrong, is the entire affine registration being done manually ? If so, as
	mentioned above the accuracy of the human can be bad.
}

Similarity transformations are used for all of the experiments in this section, and
therefore two manually selected outer eye corners are sufficient.  We have
made this clear by adding a footnote.

{\bf
	Some comments on this would be useful to better understand the results shown in table 1 and 2.
}

The discussion of these results (seen in Table 2 and 3 in the new
draft) has been updated. For the single gallery image FERET
experiment (Table 3), our algorithm is not designed for
this scenario, and it fails when the illumination changes and other
variations seriously violate its assumptions (this was true in the previous
revision as well). We found that LBP
with manual alignment achieves the best performance in this
experiment.

\end{document}
